##### Example Application:
1. [[Slot filling]]:
	Then, if neural network **have memory** to remember what it do when it comes out `leave` or `arrive`, then it maybe help neural network works

# Recurrent Neural Network(RNN)
#### Basic structure of RNN:
![[RNN.jpg|500]]
$\qquad a_1, a_2$ store the previous neural output, update in every operation and involve in next iteration.

#### Property
For the reason the next operation would consider the current result, two iterations are dependent
->** Change the sequence order would change the output**.I.e., sequence order

Applied in slot filling: 
The RNN structure would be:
![[slot_filling_4.jpg|500]]
then, two different sequence would be different output:
![[slot_filling_5.jpg|500]] 

#### Elmen Network & Jordan Network
- Elmen Network: Stock the hidden layer output.
- Jordan Network: Stork the neural network output (target).

#### Bidirectional RNN
  You can train a sequence bidirectionally.
  > Example:
  > If there is a sequence *'arrive Taipei on Nov 2nd',* and then we can train two recurrent neural networks with '*arrive Taipei on Nov 2nd*' and '*2nd Nov on Taipei arrive'*.
 
 and conbine output of two RNNs into a output layer.
 Pros: This way make RNN have more wide vision.
 
 # Long Short-term Memory(LSTM)
In LSTM, there is four gates to controll the model action:
![[LSTM.jpg|500]]
1. **Input gate**: If input gate close, the model would not receive the input data.
2. **Output gate**: If output gate close, the model would not send out the output.
3. **Forget gate**: If turn on the forget gate, it would clear the memory in memory cell

Hence, you need the input four parameters $\{x,\omega_{input}, \omega_{output}, \omega_{forget}\}$ to controll the model activity.

#### Basic model:
![[LSTM1.jpg|300]]

In this graph, $z$ is represent the input data, $z_i,z_f,z_O$ are represented the parameters in input, forget, output gate specifically; function $f$ is an indicator function.
($\{z,z_i, z_o, z_f\}$ are generated by linear combination from the previous layer input.)
- Activation: Usually sigmoid function ( We want to keep scalar between 0 and 1 -> To mimic gate open or close.)
- Three black points in the graph from bottom to top are mean:
	1. $g(z)f(z_i)$: If $f(z_i)=0$ -> It would sent 0 to the memory cell.
	2. $cf(z_f)$: If $f(z_f)=0$ -> the forgot gate close then we clear the memory cell and return 0.
	3. $h(c')f(z_o)$: where $c'=g(z)f(z_i)+cf(z_f)$ and if $f(z_0)=0$ it would sent 0 to the output.

#### How we use LSTM in network
Original network would be a network like
![[LSTM2.jpg|200]]
then, if we want to apply LSTM into network just replace the neuron into LSTM model like
![[LSTM3.jpg|400]]
then if we apply LSTM, the *neuron input size would be 4 times larger than normal network*.

Then, we can illusterate the simple operation
![[LSTM4.jpg|400]]
Practically, we not only take $x_t$ to be input layer, we want to take previous information as consideration, like 
![[LSTM5.jpg|500]]
where $c^{t-1}$ is previous memory cell, and $h^{t-1}$ is the previous output.
then, the mutliple-layer LSTM format would be like
![[LSTM6.jpg|500]]